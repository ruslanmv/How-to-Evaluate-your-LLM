{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925cb21c-d187-45f7-8d3f-642e401d98d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# For reading credentials from the .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# WML python SDK\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes, DecodingMethods\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "\n",
    "# Important: hardcoding the API key in Python code is not a best practice. We are using\n",
    "# this approach for the ease of demo setup. In a production application these variables\n",
    "# can be stored in an .env or a properties file\n",
    "\n",
    "# URL of the hosted LLMs is hardcoded because at this time all LLMs share the same endpoint\n",
    "url = \"https://us-south.ml.cloud.ibm.com\"\n",
    "\n",
    "# These global variables will be updated in get_credentials() function\n",
    "watsonx_project_id = \"\"\n",
    "# Replace with your IBM Cloud key\n",
    "api_key = \"\"\n",
    "\n",
    "def get_credentials():\n",
    "\n",
    "    load_dotenv()\n",
    "    # Update the global variables that will be used for authentication in another function\n",
    "    globals()[\"api_key\"] = os.getenv(\"api_key\", None)\n",
    "    globals()[\"watsonx_project_id\"] = os.getenv(\"project_id\", None)\n",
    "\n",
    "# The get_model function creates an LLM model object with the specified parameters\n",
    "\n",
    "def get_model(model_type,max_tokens,min_tokens,decoding,temperature):\n",
    "\n",
    "    generate_params = {\n",
    "        GenParams.MAX_NEW_TOKENS: max_tokens,\n",
    "        GenParams.MIN_NEW_TOKENS: min_tokens,\n",
    "        GenParams.DECODING_METHOD: decoding,\n",
    "        GenParams.TEMPERATURE: temperature\n",
    "    }\n",
    "\n",
    "    model = Model(\n",
    "        model_id=model_type,\n",
    "        params=generate_params,\n",
    "        credentials={\n",
    "            \"apikey\": api_key,\n",
    "            \"url\": url\n",
    "        },\n",
    "        project_id=watsonx_project_id\n",
    "        )\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_lang_chain_model(model_type,max_tokens,min_tokens,decoding,temperature):\n",
    "\n",
    "    base_model = get_model(model_type,max_tokens,min_tokens,decoding,temperature)\n",
    "    langchain_model = WatsonxLLM(model=base_model)\n",
    "\n",
    "    return langchain_model\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Get the API key and project id and update global variables\n",
    "    get_credentials()\n",
    "\n",
    "    # Test answering questions based on the provided .pdf file\n",
    "    # question = \"What is Generative AI?\"\n",
    "    # question = \"What does it take to build a generative AI model?\"\n",
    "    question = \"What are the limitations of generative AI models?\"\n",
    "    # Provide the path relative to the dir in which the script is running\n",
    "    # In this example the .pdf file is in the same directory\n",
    "    file_path = \"./Generative_AI_Overview.pdf\"\n",
    "\n",
    "    answer_questions_from_doc(api_key, watsonx_project_id, file_path, question)\n",
    "\n",
    "def answer_questions_from_doc(request_api_key, request_project_id, file_path, question):\n",
    "\n",
    "    # Update the global variable\n",
    "    globals()[\"api_key\"] = request_api_key\n",
    "    globals()[\"watsonx_project_id\"] = request_project_id\n",
    "\n",
    "    # Specify model parameters\n",
    "    model_type = \"meta-llama/llama-2-70b-chat\"\n",
    "    max_tokens = 300\n",
    "    min_tokens = 100\n",
    "    decoding = DecodingMethods.GREEDY\n",
    "    temperature = 0.7\n",
    "\n",
    "    # Get the watsonx model that can be used with LangChain\n",
    "    model = get_lang_chain_model(model_type, max_tokens, min_tokens, decoding, temperature)\n",
    "\n",
    "    loaders = [PyPDFLoader(file_path)]\n",
    "\n",
    "    index = VectorstoreIndexCreator(\n",
    "        embedding=HuggingFaceEmbeddings(),\n",
    "        text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)).from_loaders(loaders)\n",
    "\n",
    "    chain = RetrievalQA.from_chain_type(llm=model,\n",
    "                                        chain_type=\"stuff\",\n",
    "                                        retriever=index.vectorstore.as_retriever(),\n",
    "                                        input_key=\"question\")\n",
    "\n",
    "    # Invoke the chain\n",
    "    response_text = chain.invoke(question)\n",
    "\n",
    "    # print model response\n",
    "    print(\"--------------------------------- Generated response -----------------------------------\")\n",
    "    print(response_text)\n",
    "    print(\"*********************************************************************************************\")\n",
    "\n",
    "    return response_text\n",
    "\n",
    "# Invoke the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

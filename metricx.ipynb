{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372b366f-8059-4727-b734-6bf8f0911481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reading credentials from the .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# WML python SDK\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes, DecodingMethods\n",
    "# For invocation of LLM with REST API\n",
    "import requests, json\n",
    "from ibm_cloud_sdk_core import IAMTokenManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5db98b6-5da1-4a8c-a3ba-4ab54607671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the hosted LLMs is hardcoded because at this time all LLMs share the same endpoint\n",
    "url = \"https://us-south.ml.cloud.ibm.com\"\n",
    "def get_credentials():\n",
    "    load_dotenv()\n",
    "    # Update the global variables that will be used for authentication in another function\n",
    "    globals()[\"api_key\"] = os.getenv(\"api_key\", None)\n",
    "    globals()[\"watsonx_project_id\"] = os.getenv(\"project_id\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4dcb47-69de-420f-aa27-f30fef12c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The get_model function creates an LLM model object with the specified parameters\n",
    "def get_model(model_type,max_tokens,min_tokens,decoding,temperature):\n",
    "    generate_params = {\n",
    "        GenParams.MAX_NEW_TOKENS: max_tokens,\n",
    "        GenParams.MIN_NEW_TOKENS: min_tokens,\n",
    "        GenParams.DECODING_METHOD: decoding,\n",
    "        GenParams.TEMPERATURE: temperature\n",
    "    }\n",
    "    model = Model(\n",
    "        model_id=model_type,\n",
    "        params=generate_params,\n",
    "        credentials={\n",
    "            \"apikey\": api_key,\n",
    "            \"url\": url\n",
    "        },\n",
    "        project_id=watsonx_project_id\n",
    "        )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cb4a58-c660-4832-bea4-cbddb57bc63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('customer_support_tickets.csv')\n",
    "#https://www.kaggle.com/datasets/suraj520/customer-support-ticket-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e520cd69-956b-48e6-97e9-d868a0f4ddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting rows based on condition\n",
    "dataframe = df.loc[df['Ticket Status'] == \"Closed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6e54f6-222b-473e-bfd5-d963db62cf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa993a1d-f25e-436b-95bf-f92df5f55719",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[[\"Ticket Description\",\"Resolution\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b558d5b8-6591-47af-a5ff-cf67864505fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "current_working_directory = os.getcwd()\n",
    "model_path = os.path.join(current_working_directory, \"all-MiniLM-L6-v2\", \"sentence_transformers\")\n",
    "model = SentenceTransformer(model_path, cache_folder=True)  # Set cache_folder to None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b65933b-bd87-43c6-b3a6-0d0f33c33642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2219a9-e048-488b-a079-7d87b39fd20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def load_sentence_transformer_model(model_name):\n",
    "    model_path = os.path.join(\".\", model_name)\n",
    "    #model_path = os.path.join(current_working_directory,model_name, \"sentence_transformers\")\n",
    "    print(model_path)\n",
    "    return SentenceTransformer(model_path)\n",
    "\n",
    "#model_name = 'paraphrase-MiniLM-L6-v2'\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "model = load_sentence_transformer_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3be87a-4e21-4b5a-932a-9551333c1f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def load_sentence_transformer_model(model_name):\n",
    "    current_working_directory = os.getcwd()\n",
    "    model_path = os.path.join(current_working_directory, model_name, \"sentence_transformers\")\n",
    "\n",
    "    if os.path.exists(model_path):\n",
    "        return SentenceTransformer(model_path)\n",
    "    else:\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            model = AutoModel.from_pretrained(model_name)\n",
    "            os.makedirs(model_path, exist_ok=True)\n",
    "            tokenizer.save_pretrained(model_path)\n",
    "            model.save_pretrained(model_path)\n",
    "            return SentenceTransformer(model_path)\n",
    "        except Exception as e:\n",
    "            print(\"Error downloading the model from Hugging Face Hub:\", e)\n",
    "            raise\n",
    "\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "model = load_sentence_transformer_model(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2f7fc4-342e-43d3-be75-75f85845886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "current_working_directory = os.getcwd()\n",
    "def load_sentence_transformer_model(model_name):\n",
    "    model_path = os.path.join(\".\", model_name)\n",
    "    #model_path = os.path.join(current_working_directory,model_name, \"sentence_transformers\")\n",
    "    # Check if the model exists locally\n",
    "    if not os.path.exists(model_path):\n",
    "        # Download the model using Hugging Face Hub\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "        tokenizer.save_pretrained(model_path)\n",
    "        model.save_pretrained(model_path)\n",
    "\n",
    "    return SentenceTransformer(model_path)\n",
    "\n",
    "#model_name = 'sentence-transformers/paraphrase-MiniLM-L6-v2'\n",
    "model_name='all-MiniLM-L6-v2'\n",
    "model = load_sentence_transformer_model(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c1fe96-98c1-4a7c-bbd9-0074fba7125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "data = {'Ticket Description': ['Issue with the login', 'Cannot access the website', 'Error in the payment system'],\n",
    "        'Resolution': ['Reset your password', 'Check your internet connection', 'Contact the support team']}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize the SentenceTransformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Create the vector database\n",
    "ticket_descriptions = df['Ticket Description'].tolist()\n",
    "ticket_vectors = model.encode(ticket_descriptions)\n",
    "\n",
    "# Perform a similarity search for a given sentence\n",
    "def search_ticket_resolution(query):\n",
    "    query_vector = model.encode([query])\n",
    "    similarities = util.pytorch_cos_sim(query_vector, ticket_vectors)\n",
    "    most_similar_ticket_index = similarities[0].argmax()\n",
    "    return df.iloc[most_similar_ticket_index]['Resolution']\n",
    "\n",
    "query = \"I can't log in to my account\"\n",
    "resolution = search_ticket_resolution(query)\n",
    "print(\"Suggested Resolution:\", resolution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a8b1f3-d707-456c-8c47-101fedc7eb03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c2da4a-654c-4054-9f38-9ade39f3a66f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45898235-4ab0-4efb-ba0a-b45dbc7f0d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919b6896-9392-46ad-a940-90490592674c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb40780-547c-4267-ac77-bfc3ca77f53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = [PyPDFLoader(file_path)]\n",
    "\n",
    "index = VectorstoreIndexCreator(\n",
    "    embedding=HuggingFaceEmbeddings(),\n",
    "    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)).from_loaders(loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4432b1f-8821-4a79-a81a-7fa7e599e982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980ce3e1-54a5-432a-8af8-29c5c1976ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f7ea6f-f6c1-46f7-99a5-9f8508cc2eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5321e63e-056e-45b8-aca9-bb84a4978be6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9bfe05-66ce-41d9-93c1-c69e16cdb0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_complaints():\n",
    "    model_type = ModelTypes.MPT_7B_INSTRUCT2\n",
    "    max_tokens = 100\n",
    "    min_tokens = 50\n",
    "    decoding = DecodingMethods.GREEDY\n",
    "    # Temperature will be ignored if GREEDY is used\n",
    "    temperature = 0.7\n",
    "    # Instantiate the model\n",
    "    model = get_model(model_type,max_tokens,min_tokens,decoding, temperature)\n",
    "    complaint = f\"\"\"\n",
    "            I just tried to book a flight on your incredibly slow website.  All \n",
    "            the times and prices were confusing.  I liked being able to compare \n",
    "            the amenities in economy with business class side by side.  But I \n",
    "            never got to reserve a seat because I didn't understand the seat map.  \n",
    "            Next time, I'll use a travel agent!\n",
    "            \"\"\"\n",
    "    prompt_get_complaints = f\"\"\"\n",
    "    From the following customer complaint, extract 3 factors that caused the customer to be unhappy. \n",
    "    Put each factor on a new line. \n",
    "    Customer complaint:{complaint}\n",
    "    Numbered list of all the factors that caused the customer to be unhappy:\n",
    "    \"\"\"\n",
    "    # Invoke the model and print the results\n",
    "    generated_response = model.generate(prompt=prompt_get_complaints)\n",
    "    # WML API returns a dictionary object. Generated response is a list object that contains generated text\n",
    "    # as well as several other items such as token count and seed\n",
    "    # We recommmend that you put a breakpoint on this line and example the result object\n",
    "    print(\"---------------------------------------------------------------------------\")\n",
    "    print(\"Prompt: \" + prompt_get_complaints)\n",
    "    print(\"List of complaints: \" + generated_response['results'][0]['generated_text'])\n",
    "    print(\"---------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c43f55a-433d-4fbb-8617-fd42167e358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_list_of_complaints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1790f1-1dc4-400e-9bb1-c77274d9df86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71615e7-1482-49eb-988c-dca1c45894c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448aa01e-1d5c-4dd6-b630-4e72655c32e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_questions():\n",
    "    final_prompt = \"Write a paragraph about the capital of France.\"\n",
    "    model_type = ModelTypes.FLAN_UL2\n",
    "    max_tokens = 300\n",
    "    min_tokens = 50\n",
    "    decoding = DecodingMethods.SAMPLE\n",
    "    temperature = 0.7\n",
    "    # Instantiate the model\n",
    "    model = get_model(model_type,max_tokens,min_tokens,decoding, temperature)\n",
    "    # Invoke the model and print the results\n",
    "    generated_response = model.generate(prompt=final_prompt)\n",
    "    # WML API returns a dictionary object. Generated response is a list object that contains generated text\n",
    "\n",
    "    print(\"---------------------------------------------------------------------------\")\n",
    "    print(\"Question/request: \" + final_prompt)\n",
    "    print(\"Answer: \" + generated_response['results'][0]['generated_text'])\n",
    "    print(\"---------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b46f64e-3359-4a46-9436-b0fca69eae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_with_REST():\n",
    "    rest_url =\"https://us-south.ml.cloud.ibm.com/ml/v1-beta/generation/text?version=2023-05-29\"\n",
    "    access_token = get_auth_token()\n",
    "    model_type = \"google/flan-ul2\"\n",
    "    max_tokens = 300\n",
    "    min_tokens = 50\n",
    "    decoding = \"sample\"\n",
    "    temperature = 0.7\n",
    "\n",
    "    final_prompt = \"Write a paragraph about the capital of France.\"\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer \" + access_token\n",
    "        }\n",
    "\n",
    "    data = {\n",
    "        \"model_id\": model_type,\n",
    "        \"input\": final_prompt,\n",
    "        \"parameters\": {\n",
    "            \"decoding_method\": decoding,\n",
    "            \"max_new_tokens\": max_tokens,\n",
    "            \"min_new_tokens\": min_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"stop_sequences\": [\".\"],\n",
    "            },\n",
    "        \"project_id\": watsonx_project_id\n",
    "    }\n",
    "    response = requests.post(rest_url, headers=headers, data=json.dumps(data))\n",
    "    generated_response = response.json()['results'][0]['generated_text']\n",
    "\n",
    "    print(\"--------------------------Invocation with REST-------------------------------------------\")\n",
    "    print(\"Question/request: \" + final_prompt)\n",
    "    print(\"Answer: \" + generated_response)\n",
    "    print(\"---------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f9b488-b319-44b3-8873-1605a35e025e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auth_token():\n",
    "\n",
    "    # Access token is required for REST invocation of the LLM\n",
    "    access_token = IAMTokenManager(apikey=api_key,url=\"https://iam.cloud.ibm.com/identity/token\").get_token()\n",
    "    return access_token\n",
    "\n",
    "def demo_LLM_invocation():\n",
    "    # Load the api key and project id\n",
    "    get_credentials()\n",
    "    # Show examples of 2 use cases/prompts\n",
    "    answer_questions()\n",
    "    get_list_of_complaints()\n",
    "    # Simple prompt - invoked with the REST API\n",
    "    invoke_with_REST()\n",
    "demo_LLM_invocation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f737b917-4619-4c6c-95f1-1be060adfe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reading credentials from the .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb.api.types import EmbeddingFunction\n",
    "\n",
    "# WML python SDK\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes, DecodingMethods\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import chromadb\n",
    "import en_core_web_md\n",
    "\n",
    "# Important: hardcoding the API key in Python code is not a best practice. We are using\n",
    "# this approach for the ease of demo setup. In a production application these variables\n",
    "# can be stored in an .env or a properties file\n",
    "\n",
    "# URL of the hosted LLMs is hardcoded because at this time all LLMs share the same endpoint\n",
    "url = \"https://us-south.ml.cloud.ibm.com\"\n",
    "\n",
    "# These global variables will be updated in get_credentials() function\n",
    "watsonx_project_id = \"\"\n",
    "# Replace with your IBM Cloud key\n",
    "api_key = \"\"\n",
    "\n",
    "def get_credentials():\n",
    "\n",
    "    load_dotenv()\n",
    "    # Update the global variables that will be used for authentication in another function\n",
    "    globals()[\"api_key\"] = os.getenv(\"api_key\", None)\n",
    "    globals()[\"watsonx_project_id\"] = os.getenv(\"project_id\", None)\n",
    "\n",
    "# The get_model function creates an LLM model object with the specified parameters\n",
    "\n",
    "def get_model(model_type, max_tokens, min_tokens, decoding, temperature, top_k, top_p):\n",
    "    generate_params = {\n",
    "        GenParams.MAX_NEW_TOKENS: max_tokens,\n",
    "        GenParams.MIN_NEW_TOKENS: min_tokens,\n",
    "        GenParams.DECODING_METHOD: decoding,\n",
    "        GenParams.TEMPERATURE: temperature,\n",
    "        GenParams.TOP_K: top_k,\n",
    "        GenParams.TOP_P: top_p,\n",
    "    }\n",
    "\n",
    "    model = Model(\n",
    "        model_id=model_type,\n",
    "        params=generate_params,\n",
    "        credentials={\n",
    "            \"apikey\": api_key,\n",
    "            \"url\": url\n",
    "        },\n",
    "        project_id=watsonx_project_id\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model_test(model_type, max_tokens, min_tokens, decoding, temperature):\n",
    "    generate_params = {\n",
    "        GenParams.MAX_NEW_TOKENS: max_tokens,\n",
    "        GenParams.MIN_NEW_TOKENS: min_tokens,\n",
    "        GenParams.DECODING_METHOD: decoding,\n",
    "        GenParams.TEMPERATURE: temperature\n",
    "    }\n",
    "\n",
    "    model = Model(\n",
    "        model_id=model_type,\n",
    "        params=generate_params,\n",
    "        credentials={\n",
    "            \"apikey\": api_key,\n",
    "            \"url\": url\n",
    "        },\n",
    "        project_id=watsonx_project_id\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Embedding function\n",
    "class MiniLML6V2EmbeddingFunction(EmbeddingFunction):\n",
    "    MODEL = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    def __call__(self, texts):\n",
    "        return MiniLML6V2EmbeddingFunction.MODEL.encode(texts).tolist()\n",
    "\n",
    "\n",
    "def extract_text(url):\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page using BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Extract contents of <p> elements\n",
    "            p_contents = [p.get_text() for p in soup.find_all('p')]\n",
    "\n",
    "            # Print the contents of <p> elements\n",
    "            print(\"\\nContents of <p> elements: \\n\")\n",
    "            for content in p_contents:\n",
    "                print(content)\n",
    "            raw_web_text = \" \".join(p_contents)\n",
    "            # remove \\xa0 which is used in html to avoid words break acorss lines.\n",
    "            cleaned_text = raw_web_text.replace(\"\\xa0\", \" \")\n",
    "            return cleaned_text\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "\n",
    "def split_text_into_sentences(text):\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    cleaned_sentences = [s.strip() for s in sentences]\n",
    "    return cleaned_sentences\n",
    "\n",
    "\n",
    "def create_embedding(url, collection_name):\n",
    "    cleaned_text = extract_text(url)\n",
    "    cleaned_sentences = split_text_into_sentences(cleaned_text)\n",
    "\n",
    "    client = chromadb.Client()\n",
    "\n",
    "    collection = client.get_or_create_collection(collection_name)\n",
    "\n",
    "    # Upload text to chroma\n",
    "    collection.upsert(\n",
    "        documents=cleaned_sentences,\n",
    "        metadatas=[{\"source\": str(i)} for i in range(len(cleaned_sentences))],\n",
    "        ids=[str(i) for i in range(len(cleaned_sentences))],\n",
    "    )\n",
    "\n",
    "    return collection\n",
    "\n",
    "\n",
    "def create_prompt(url, question, collection_name):\n",
    "    # Create embeddings for the text file\n",
    "    collection = create_embedding(url, collection_name)\n",
    "\n",
    "    # query relevant information\n",
    "    relevant_chunks = collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=5,\n",
    "    )\n",
    "    context = \"\\n\\n\\n\".join(relevant_chunks[\"documents\"][0])\n",
    "    # Please note that this is a generic format. You can change this format to be specific to llama\n",
    "    prompt = (f\"{context}\\n\\nPlease answer the following question in one sentence using this \"\n",
    "              + f\"text. \"\n",
    "              + f\"If the question is unanswerable, say \\\"unanswerable\\\". Do not include information that's not relevant to the question.\"\n",
    "              + f\"Question: {question}\")\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Get the API key and project id and update global variables\n",
    "    get_credentials()\n",
    "\n",
    "    # Try diffrent URLs and questions\n",
    "    url = \"https://www.usbank.com/financialiq/manage-your-household/buy-a-car/own-electric-vehicles-learned-buying-driving-EVs.html\"\n",
    "\n",
    "    question = \"What are the incentives for purchasing EVs?\"\n",
    "    # question = \"What is the percentage of driving powered by hybrid cars?\"\n",
    "    # question = \"Can an EV be plugged in to a household outlet?\"\n",
    "    collection_name = \"test_web_RAG\"\n",
    "\n",
    "    answer_questions_from_web(api_key, watsonx_project_id, url, question, collection_name)\n",
    "\n",
    "\n",
    "def answer_questions_from_web(request_api_key, request_project_id, url, question, collection_name):\n",
    "    # Update the global variable\n",
    "    globals()[\"api_key\"] = request_api_key\n",
    "    globals()[\"watsonx_project_id\"] = request_project_id\n",
    "\n",
    "    # Specify model parameters\n",
    "    model_type = \"meta-llama/llama-2-70b-chat\"\n",
    "    max_tokens = 100\n",
    "    min_tokens = 50\n",
    "    top_k = 50\n",
    "    top_p = 1\n",
    "    decoding = DecodingMethods.GREEDY\n",
    "    temperature = 0.7\n",
    "\n",
    "    # Get the watsonx model = try both options\n",
    "    model = get_model(model_type, max_tokens, min_tokens, decoding, temperature, top_k, top_p)\n",
    "\n",
    "    # Get the prompt\n",
    "    complete_prompt = create_prompt(url, question, collection_name)\n",
    "\n",
    "    # Let's review the prompt\n",
    "    print(\"----------------------------------------------------------------------------------------------------\")\n",
    "    print(\"*** Prompt:\" + complete_prompt + \"***\")\n",
    "    print(\"----------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    generated_response = model.generate(prompt=complete_prompt)\n",
    "    response_text = generated_response['results'][0]['generated_text']\n",
    "\n",
    "    # Remove trailing white spaces\n",
    "    response_text = response_text.strip()\n",
    "\n",
    "    # print model response\n",
    "    print(\"--------------------------------- Generated response -----------------------------------\")\n",
    "    print(response_text)\n",
    "    print(\"*********************************************************************************************\")\n",
    "\n",
    "    return response_text\n",
    "\n",
    "\n",
    "# Invoke the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MetricX)",
   "language": "python",
   "name": "metricx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
